{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RBi_nxq4rr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "babr3EFB5cYU",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6f254e23-afaa-44d2-8027-773d6d77bf80"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-322e0a77-13f1-4c7c-b181-b5fc2378d93a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-322e0a77-13f1-4c7c-b181-b5fc2378d93a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Cluster1Mondayn.csv to Cluster1Mondayn.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F73Ojk9AVKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OEeo0ek5WzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Data = pd.read_csv('Cluster1Mondayn.csv', index_col='Time', parse_dates=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn8L6sMd5Zp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = Data.drop(['dayofyear', 'month', 'dayofweek', 'hourofday', 'minuteofday', 'daytype', 'season'], axis=1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1n5YKLa5omu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_sequence(sequence, n_steps):\n",
        "    X, Y = list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        \n",
        "        end_ix = i + n_steps\n",
        "        \n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        X.append(seq_x) \n",
        "        Y.append(seq_y)\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "n_steps = 3\n",
        "X, Y = split_sequence(data.values, n_steps)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDpYB5qB5u-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Energy_Series(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx]\n",
        "        y = self.targets[idx]\n",
        "        \n",
        "        return x, y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG2N0YY35wPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 64, 1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(64, 50)\n",
        "\n",
        "        self.hidden_size = 128\n",
        "        self.num_layers = 2\n",
        "        self.lstm = nn.LSTM(1, 128, 2, batch_first=True)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = x.view(-1, 64)\n",
        "        x = self.fc1(x)\n",
        "        x = x.unsqueeze(2)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        \n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        \n",
        "        out = self.fc2(out[:, -1, :])\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9aiQqye7lG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CNN_LSTM().to(device).float()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQPjUTvB58L5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = Energy_Series(X.reshape(X.shape[0],X.shape[1],1),Y)\n",
        "train_loader = torch.utils.data.DataLoader(train,batch_size=2,shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hExpK4L5_D_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_losses = []\n",
        "def Train():\n",
        "    running_loss = 0\n",
        "    \n",
        "    model.train() \n",
        "    \n",
        "    for idx, (inputs,labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device).float()\n",
        "        labels = labels.to(device).float()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(inputs)\n",
        "        loss = criterion(preds,labels)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        #del loss, inputs, labels, preds, \n",
        "        \n",
        "    train_loss = running_loss/len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    print(f'train_loss {train_loss}') "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuOFhcTuA_ri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecaed562-5648-49ac-8596-373bf3677e77"
      },
      "source": [
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    print('epochs {}/{}'.format(epoch+1,epochs))\n",
        "    Train()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs 1/500\n",
            "train_loss 0.0175387088794322\n",
            "epochs 2/500\n",
            "train_loss 0.012388019205333438\n",
            "epochs 3/500\n",
            "train_loss 0.0020222417545081757\n",
            "epochs 4/500\n",
            "train_loss 0.0018842501227757447\n",
            "epochs 5/500\n",
            "train_loss 0.001873607779970121\n",
            "epochs 6/500\n",
            "train_loss 0.001821986314628196\n",
            "epochs 7/500\n",
            "train_loss 0.001738999502977261\n",
            "epochs 8/500\n",
            "train_loss 0.0016614411978793174\n",
            "epochs 9/500\n",
            "train_loss 0.001586282314115019\n",
            "epochs 10/500\n",
            "train_loss 0.0015242117595126235\n",
            "epochs 11/500\n",
            "train_loss 0.0014652779639169312\n",
            "epochs 12/500\n",
            "train_loss 0.0014160470785585214\n",
            "epochs 13/500\n",
            "train_loss 0.0013654002385846446\n",
            "epochs 14/500\n",
            "train_loss 0.001328104303224626\n",
            "epochs 15/500\n",
            "train_loss 0.0012906818899000065\n",
            "epochs 16/500\n",
            "train_loss 0.0012588564521380742\n",
            "epochs 17/500\n",
            "train_loss 0.0012323465303417148\n",
            "epochs 18/500\n",
            "train_loss 0.0012065927797170841\n",
            "epochs 19/500\n",
            "train_loss 0.0011849226617126286\n",
            "epochs 20/500\n",
            "train_loss 0.0011619028325340521\n",
            "epochs 21/500\n",
            "train_loss 0.0011453267276887243\n",
            "epochs 22/500\n",
            "train_loss 0.001127412463050709\n",
            "epochs 23/500\n",
            "train_loss 0.0011127120329396615\n",
            "epochs 24/500\n",
            "train_loss 0.0010986359251772115\n",
            "epochs 25/500\n",
            "train_loss 0.0010871678646355732\n",
            "epochs 26/500\n",
            "train_loss 0.0010769118294698924\n",
            "epochs 27/500\n",
            "train_loss 0.0010677608096457442\n",
            "epochs 28/500\n",
            "train_loss 0.0010589108258641624\n",
            "epochs 29/500\n",
            "train_loss 0.0010514346166972134\n",
            "epochs 30/500\n",
            "train_loss 0.0010433626389989843\n",
            "epochs 31/500\n",
            "train_loss 0.0010364028752072828\n",
            "epochs 32/500\n",
            "train_loss 0.0010304907591270657\n",
            "epochs 33/500\n",
            "train_loss 0.0010237324681763777\n",
            "epochs 34/500\n",
            "train_loss 0.0010177074235252553\n",
            "epochs 35/500\n",
            "train_loss 0.0010120128888590446\n",
            "epochs 36/500\n",
            "train_loss 0.001006578659988269\n",
            "epochs 37/500\n",
            "train_loss 0.0010017552430439264\n",
            "epochs 38/500\n",
            "train_loss 0.0009963367084881072\n",
            "epochs 39/500\n",
            "train_loss 0.0009919961690961889\n",
            "epochs 40/500\n",
            "train_loss 0.00098852403124254\n",
            "epochs 41/500\n",
            "train_loss 0.000984615776344036\n",
            "epochs 42/500\n",
            "train_loss 0.0009796241305893726\n",
            "epochs 43/500\n",
            "train_loss 0.000975609960382943\n",
            "epochs 44/500\n",
            "train_loss 0.0009708937731510137\n",
            "epochs 45/500\n",
            "train_loss 0.0009672854629794129\n",
            "epochs 46/500\n",
            "train_loss 0.0009638944961645052\n",
            "epochs 47/500\n",
            "train_loss 0.0009607751821507448\n",
            "epochs 48/500\n",
            "train_loss 0.0009571944592848036\n",
            "epochs 49/500\n",
            "train_loss 0.000953630069879894\n",
            "epochs 50/500\n",
            "train_loss 0.0009506258447999198\n",
            "epochs 51/500\n",
            "train_loss 0.000947250952762469\n",
            "epochs 52/500\n",
            "train_loss 0.0009435684030496757\n",
            "epochs 53/500\n",
            "train_loss 0.0009401220909870809\n",
            "epochs 54/500\n",
            "train_loss 0.0009373770485953776\n",
            "epochs 55/500\n",
            "train_loss 0.0009342982014219798\n",
            "epochs 56/500\n",
            "train_loss 0.000931673515820294\n",
            "epochs 57/500\n",
            "train_loss 0.000929157449312332\n",
            "epochs 58/500\n",
            "train_loss 0.0009260594895541041\n",
            "epochs 59/500\n",
            "train_loss 0.000924059541770528\n",
            "epochs 60/500\n",
            "train_loss 0.0009213556995013724\n",
            "epochs 61/500\n",
            "train_loss 0.0009181716431618476\n",
            "epochs 62/500\n",
            "train_loss 0.000915918698657235\n",
            "epochs 63/500\n",
            "train_loss 0.0009133988472906713\n",
            "epochs 64/500\n",
            "train_loss 0.0009109290822083147\n",
            "epochs 65/500\n",
            "train_loss 0.0009084799274304418\n",
            "epochs 66/500\n",
            "train_loss 0.0009060796888897916\n",
            "epochs 67/500\n",
            "train_loss 0.0009033165253849664\n",
            "epochs 68/500\n",
            "train_loss 0.0009015898465920939\n",
            "epochs 69/500\n",
            "train_loss 0.0008989372989628041\n",
            "epochs 70/500\n",
            "train_loss 0.0008965585447736337\n",
            "epochs 71/500\n",
            "train_loss 0.0008938316521803293\n",
            "epochs 72/500\n",
            "train_loss 0.0008916801178715231\n",
            "epochs 73/500\n",
            "train_loss 0.0008894794883645926\n",
            "epochs 74/500\n",
            "train_loss 0.000886875953830622\n",
            "epochs 75/500\n",
            "train_loss 0.0008840423962807467\n",
            "epochs 76/500\n",
            "train_loss 0.00088217653929404\n",
            "epochs 77/500\n",
            "train_loss 0.0008796898433513899\n",
            "epochs 78/500\n",
            "train_loss 0.0008771713486585961\n",
            "epochs 79/500\n",
            "train_loss 0.0008748495282103785\n",
            "epochs 80/500\n",
            "train_loss 0.0008725070412432059\n",
            "epochs 81/500\n",
            "train_loss 0.0008706422184708857\n",
            "epochs 82/500\n",
            "train_loss 0.0008680542980105086\n",
            "epochs 83/500\n",
            "train_loss 0.0008666071315514059\n",
            "epochs 84/500\n",
            "train_loss 0.0008641607101616311\n",
            "epochs 85/500\n",
            "train_loss 0.0008622761279351359\n",
            "epochs 86/500\n",
            "train_loss 0.0008595427183286667\n",
            "epochs 87/500\n",
            "train_loss 0.0008579344166043482\n",
            "epochs 88/500\n",
            "train_loss 0.0008559278422974793\n",
            "epochs 89/500\n",
            "train_loss 0.0008533843615985844\n",
            "epochs 90/500\n",
            "train_loss 0.0008520821259612372\n",
            "epochs 91/500\n",
            "train_loss 0.0008489179951086795\n",
            "epochs 92/500\n",
            "train_loss 0.00084708846888764\n",
            "epochs 93/500\n",
            "train_loss 0.0008449058882090224\n",
            "epochs 94/500\n",
            "train_loss 0.0008426727369098938\n",
            "epochs 95/500\n",
            "train_loss 0.0008400820307387889\n",
            "epochs 96/500\n",
            "train_loss 0.0008383110851713871\n",
            "epochs 97/500\n",
            "train_loss 0.0008354414612639224\n",
            "epochs 98/500\n",
            "train_loss 0.0008331575036873305\n",
            "epochs 99/500\n",
            "train_loss 0.0008318541105293754\n",
            "epochs 100/500\n",
            "train_loss 0.0008296319452583731\n",
            "epochs 101/500\n",
            "train_loss 0.0008280654363882587\n",
            "epochs 102/500\n",
            "train_loss 0.0008260065488392601\n",
            "epochs 103/500\n",
            "train_loss 0.0008243360468252409\n",
            "epochs 104/500\n",
            "train_loss 0.0008220739057768683\n",
            "epochs 105/500\n",
            "train_loss 0.0008199450624510512\n",
            "epochs 106/500\n",
            "train_loss 0.0008181314497749625\n",
            "epochs 107/500\n",
            "train_loss 0.0008181586627679346\n",
            "epochs 108/500\n",
            "train_loss 0.0008143061935350379\n",
            "epochs 109/500\n",
            "train_loss 0.0008149098257150127\n",
            "epochs 110/500\n",
            "train_loss 0.0008146065466561542\n",
            "epochs 111/500\n",
            "train_loss 0.0008121791181286542\n",
            "epochs 112/500\n",
            "train_loss 0.0008088881180062424\n",
            "epochs 113/500\n",
            "train_loss 0.0008084370761239903\n",
            "epochs 114/500\n",
            "train_loss 0.000808295564979399\n",
            "epochs 115/500\n",
            "train_loss 0.0008057641560790881\n",
            "epochs 116/500\n",
            "train_loss 0.0008034729301748833\n",
            "epochs 117/500\n",
            "train_loss 0.0008020598966981512\n",
            "epochs 118/500\n",
            "train_loss 0.0008006363083420789\n",
            "epochs 119/500\n",
            "train_loss 0.0007997439571620813\n",
            "epochs 120/500\n",
            "train_loss 0.0007978793766206047\n",
            "epochs 121/500\n",
            "train_loss 0.0007953772737570577\n",
            "epochs 122/500\n",
            "train_loss 0.0007934032022015721\n",
            "epochs 123/500\n",
            "train_loss 0.000794636514067148\n",
            "epochs 124/500\n",
            "train_loss 0.0007918087144459351\n",
            "epochs 125/500\n",
            "train_loss 0.0007902877348231394\n",
            "epochs 126/500\n",
            "train_loss 0.0007866639633205738\n",
            "epochs 127/500\n",
            "train_loss 0.0007844750228684672\n",
            "epochs 128/500\n",
            "train_loss 0.0007849918487602388\n",
            "epochs 129/500\n",
            "train_loss 0.000782164720238719\n",
            "epochs 130/500\n",
            "train_loss 0.0007804643364694996\n",
            "epochs 131/500\n",
            "train_loss 0.0007769564410205495\n",
            "epochs 132/500\n",
            "train_loss 0.0007742819710255982\n",
            "epochs 133/500\n",
            "train_loss 0.0007719327678199633\n",
            "epochs 134/500\n",
            "train_loss 0.0007700015260152293\n",
            "epochs 135/500\n",
            "train_loss 0.0007662569483611767\n",
            "epochs 136/500\n",
            "train_loss 0.0007644800170593064\n",
            "epochs 137/500\n",
            "train_loss 0.0007643643407872664\n",
            "epochs 138/500\n",
            "train_loss 0.0007602981150149794\n",
            "epochs 139/500\n",
            "train_loss 0.0007586105268854611\n",
            "epochs 140/500\n",
            "train_loss 0.0007581998255314371\n",
            "epochs 141/500\n",
            "train_loss 0.0007592383667524454\n",
            "epochs 142/500\n",
            "train_loss 0.0007538510632427103\n",
            "epochs 143/500\n",
            "train_loss 0.000761775841631029\n",
            "epochs 144/500\n",
            "train_loss 0.0007572467312247786\n",
            "epochs 145/500\n",
            "train_loss 0.000760680652470467\n",
            "epochs 146/500\n",
            "train_loss 0.0007472409112840673\n",
            "epochs 147/500\n",
            "train_loss 0.0007669271371933261\n",
            "epochs 148/500\n",
            "train_loss 0.000785924556058794\n",
            "epochs 149/500\n",
            "train_loss 0.0007536152231384722\n",
            "epochs 150/500\n",
            "train_loss 0.0007604567590255802\n",
            "epochs 151/500\n",
            "train_loss 0.0007515928901827003\n",
            "epochs 152/500\n",
            "train_loss 0.0007553657731423709\n",
            "epochs 153/500\n",
            "train_loss 0.0007451833244836193\n",
            "epochs 154/500\n",
            "train_loss 0.0007645962887769406\n",
            "epochs 155/500\n",
            "train_loss 0.0007840577354406842\n",
            "epochs 156/500\n",
            "train_loss 0.0007519041741542182\n",
            "epochs 157/500\n",
            "train_loss 0.0007572605845288269\n",
            "epochs 158/500\n",
            "train_loss 0.0007487050032674995\n",
            "epochs 159/500\n",
            "train_loss 0.0007514164713433517\n",
            "epochs 160/500\n",
            "train_loss 0.0007408108484430285\n",
            "epochs 161/500\n",
            "train_loss 0.0007665038197856445\n",
            "epochs 162/500\n",
            "train_loss 0.0007962487149491081\n",
            "epochs 163/500\n",
            "train_loss 0.0007735483405922788\n",
            "epochs 164/500\n",
            "train_loss 0.0007545923033200938\n",
            "epochs 165/500\n",
            "train_loss 0.0007516364801850095\n",
            "epochs 166/500\n",
            "train_loss 0.0007544732846967833\n",
            "epochs 167/500\n",
            "train_loss 0.0007498063310794586\n",
            "epochs 168/500\n",
            "train_loss 0.0007528451774368494\n",
            "epochs 169/500\n",
            "train_loss 0.0007465990045523696\n",
            "epochs 170/500\n",
            "train_loss 0.0007556396597003932\n",
            "epochs 171/500\n",
            "train_loss 0.0007673488368100845\n",
            "epochs 172/500\n",
            "train_loss 0.0007422644885037243\n",
            "epochs 173/500\n",
            "train_loss 0.0007580964942808951\n",
            "epochs 174/500\n",
            "train_loss 0.0007808503104148234\n",
            "epochs 175/500\n",
            "train_loss 0.0007499881108920881\n",
            "epochs 176/500\n",
            "train_loss 0.0007477739151582966\n",
            "epochs 177/500\n",
            "train_loss 0.0007466483117357118\n",
            "epochs 178/500\n",
            "train_loss 0.0007384855643469535\n",
            "epochs 179/500\n",
            "train_loss 0.0007593075557520252\n",
            "epochs 180/500\n",
            "train_loss 0.0007878604309101044\n",
            "epochs 181/500\n",
            "train_loss 0.0007668936457303664\n",
            "epochs 182/500\n",
            "train_loss 0.0007457548374194964\n",
            "epochs 183/500\n",
            "train_loss 0.0007500048124752454\n",
            "epochs 184/500\n",
            "train_loss 0.0007423168969998077\n",
            "epochs 185/500\n",
            "train_loss 0.0007329219006991434\n",
            "epochs 186/500\n",
            "train_loss 0.0007665793949427848\n",
            "epochs 187/500\n",
            "train_loss 0.0007881989855944235\n",
            "epochs 188/500\n",
            "train_loss 0.0007748658997067001\n",
            "epochs 189/500\n",
            "train_loss 0.0007652886297086703\n",
            "epochs 190/500\n",
            "train_loss 0.0007494724766327746\n",
            "epochs 191/500\n",
            "train_loss 0.0007462283461409573\n",
            "epochs 192/500\n",
            "train_loss 0.0007355576967468704\n",
            "epochs 193/500\n",
            "train_loss 0.0007438874571150747\n",
            "epochs 194/500\n",
            "train_loss 0.0007551745161063156\n",
            "epochs 195/500\n",
            "train_loss 0.0007358938765422307\n",
            "epochs 196/500\n",
            "train_loss 0.000749345058934405\n",
            "epochs 197/500\n",
            "train_loss 0.0007750342742203965\n",
            "epochs 198/500\n",
            "train_loss 0.0007487766457716349\n",
            "epochs 199/500\n",
            "train_loss 0.0007375066734399858\n",
            "epochs 200/500\n",
            "train_loss 0.0007462836738793047\n",
            "epochs 201/500\n",
            "train_loss 0.0007583893565102102\n",
            "epochs 202/500\n",
            "train_loss 0.0007326772282276702\n",
            "epochs 203/500\n",
            "train_loss 0.0007517345619086018\n",
            "epochs 204/500\n",
            "train_loss 0.0007712813827346776\n",
            "epochs 205/500\n",
            "train_loss 0.0007492128935183194\n",
            "epochs 206/500\n",
            "train_loss 0.0007378798075999885\n",
            "epochs 207/500\n",
            "train_loss 0.0007461125678487725\n",
            "epochs 208/500\n",
            "train_loss 0.0007644159289733007\n",
            "epochs 209/500\n",
            "train_loss 0.0007381054467913021\n",
            "epochs 210/500\n",
            "train_loss 0.0007396386819376366\n",
            "epochs 211/500\n",
            "train_loss 0.0007261128230727865\n",
            "epochs 212/500\n",
            "train_loss 0.0007547368724642058\n",
            "epochs 213/500\n",
            "train_loss 0.000774942234699197\n",
            "epochs 214/500\n",
            "train_loss 0.000758904352653039\n",
            "epochs 215/500\n",
            "train_loss 0.0007425373923751427\n",
            "epochs 216/500\n",
            "train_loss 0.0007359487096020845\n",
            "epochs 217/500\n",
            "train_loss 0.000738998111008092\n",
            "epochs 218/500\n",
            "train_loss 0.0007491500245809361\n",
            "epochs 219/500\n",
            "train_loss 0.0007270755300663098\n",
            "epochs 220/500\n",
            "train_loss 0.0007488197131494558\n",
            "epochs 221/500\n",
            "train_loss 0.0007671694717244338\n",
            "epochs 222/500\n",
            "train_loss 0.0007506481496849934\n",
            "epochs 223/500\n",
            "train_loss 0.0007320021247475939\n",
            "epochs 224/500\n",
            "train_loss 0.0007373497801895817\n",
            "epochs 225/500\n",
            "train_loss 0.0007254371988891623\n",
            "epochs 226/500\n",
            "train_loss 0.0007475739134029125\n",
            "epochs 227/500\n",
            "train_loss 0.000770315757058859\n",
            "epochs 228/500\n",
            "train_loss 0.0007528995480261979\n",
            "epochs 229/500\n",
            "train_loss 0.0007354728109477705\n",
            "epochs 230/500\n",
            "train_loss 0.0007337514798568563\n",
            "epochs 231/500\n",
            "train_loss 0.0007216235973774022\n",
            "epochs 232/500\n",
            "train_loss 0.0007440424157314213\n",
            "epochs 233/500\n",
            "train_loss 0.000768906567304983\n",
            "epochs 234/500\n",
            "train_loss 0.0007492958928031076\n",
            "epochs 235/500\n",
            "train_loss 0.0007301629190763338\n",
            "epochs 236/500\n",
            "train_loss 0.0007344231382452136\n",
            "epochs 237/500\n",
            "train_loss 0.0007241369837964436\n",
            "epochs 238/500\n",
            "train_loss 0.0007412513145050872\n",
            "epochs 239/500\n",
            "train_loss 0.0007603278553606806\n",
            "epochs 240/500\n",
            "train_loss 0.0007380308447782461\n",
            "epochs 241/500\n",
            "train_loss 0.0007266314563519012\n",
            "epochs 242/500\n",
            "train_loss 0.0007315368140564475\n",
            "epochs 243/500\n",
            "train_loss 0.0007347225771945086\n",
            "epochs 244/500\n",
            "train_loss 0.0007277703161158562\n",
            "epochs 245/500\n",
            "train_loss 0.0007159313867370859\n",
            "epochs 246/500\n",
            "train_loss 0.0007570023113823193\n",
            "epochs 247/500\n",
            "train_loss 0.0007668824413731587\n",
            "epochs 248/500\n",
            "train_loss 0.0007570114509569761\n",
            "epochs 249/500\n",
            "train_loss 0.0007479578161413173\n",
            "epochs 250/500\n",
            "train_loss 0.0007335240152160932\n",
            "epochs 251/500\n",
            "train_loss 0.0007277522296199539\n",
            "epochs 252/500\n",
            "train_loss 0.0007159431725575499\n",
            "epochs 253/500\n",
            "train_loss 0.0007411015003051919\n",
            "epochs 254/500\n",
            "train_loss 0.0007628442576131301\n",
            "epochs 255/500\n",
            "train_loss 0.0007455936779524736\n",
            "epochs 256/500\n",
            "train_loss 0.000724979119093783\n",
            "epochs 257/500\n",
            "train_loss 0.0007301486815795066\n",
            "epochs 258/500\n",
            "train_loss 0.0007248449712958358\n",
            "epochs 259/500\n",
            "train_loss 0.0007314277066175868\n",
            "epochs 260/500\n",
            "train_loss 0.0007438901667144447\n",
            "epochs 261/500\n",
            "train_loss 0.0007198679591824793\n",
            "epochs 262/500\n",
            "train_loss 0.0007349234425916795\n",
            "epochs 263/500\n",
            "train_loss 0.0007517486557373672\n",
            "epochs 264/500\n",
            "train_loss 0.0007293678639183633\n",
            "epochs 265/500\n",
            "train_loss 0.0007250505187587199\n",
            "epochs 266/500\n",
            "train_loss 0.0007147670768234736\n",
            "epochs 267/500\n",
            "train_loss 0.0007217051339747557\n",
            "epochs 268/500\n",
            "train_loss 0.0007356383631177284\n",
            "epochs 269/500\n",
            "train_loss 0.0007191283480632744\n",
            "epochs 270/500\n",
            "train_loss 0.0007079165265807881\n",
            "epochs 271/500\n",
            "train_loss 0.0007631463055717664\n",
            "epochs 272/500\n",
            "train_loss 0.0007598567189996129\n",
            "epochs 273/500\n",
            "train_loss 0.0007512738471050038\n",
            "epochs 274/500\n",
            "train_loss 0.0007428722802733869\n",
            "epochs 275/500\n",
            "train_loss 0.000728590941122871\n",
            "epochs 276/500\n",
            "train_loss 0.0007198989226795375\n",
            "epochs 277/500\n",
            "train_loss 0.0007219213471117827\n",
            "epochs 278/500\n",
            "train_loss 0.0007301286199885926\n",
            "epochs 279/500\n",
            "train_loss 0.0007196904991726642\n",
            "epochs 280/500\n",
            "train_loss 0.0007082795084976122\n",
            "epochs 281/500\n",
            "train_loss 0.0007520889441305287\n",
            "epochs 282/500\n",
            "train_loss 0.0007570186874390173\n",
            "epochs 283/500\n",
            "train_loss 0.0007470772920903141\n",
            "epochs 284/500\n",
            "train_loss 0.0007359621019136024\n",
            "epochs 285/500\n",
            "train_loss 0.0007220486834922514\n",
            "epochs 286/500\n",
            "train_loss 0.0007215430478194353\n",
            "epochs 287/500\n",
            "train_loss 0.0007228306188716815\n",
            "epochs 288/500\n",
            "train_loss 0.0007215096865019127\n",
            "epochs 289/500\n",
            "train_loss 0.000729194162419986\n",
            "epochs 290/500\n",
            "train_loss 0.000714376109168149\n",
            "epochs 291/500\n",
            "train_loss 0.0007125004143972186\n",
            "epochs 292/500\n",
            "train_loss 0.000705267662717467\n",
            "epochs 293/500\n",
            "train_loss 0.0007651195900375432\n",
            "epochs 294/500\n",
            "train_loss 0.0007523494906405561\n",
            "epochs 295/500\n",
            "train_loss 0.0007437024613301969\n",
            "epochs 296/500\n",
            "train_loss 0.000733709335370232\n",
            "epochs 297/500\n",
            "train_loss 0.0007190637541683422\n",
            "epochs 298/500\n",
            "train_loss 0.0007138397930984737\n",
            "epochs 299/500\n",
            "train_loss 0.0007054851306019771\n",
            "epochs 300/500\n",
            "train_loss 0.0007329992922923172\n",
            "epochs 301/500\n",
            "train_loss 0.0007558209125616917\n",
            "epochs 302/500\n",
            "train_loss 0.0007401945553431882\n",
            "epochs 303/500\n",
            "train_loss 0.0007232315088516024\n",
            "epochs 304/500\n",
            "train_loss 0.0007145813599568427\n",
            "epochs 305/500\n",
            "train_loss 0.0007029341457017072\n",
            "epochs 306/500\n",
            "train_loss 0.0007315644438112615\n",
            "epochs 307/500\n",
            "train_loss 0.0007550574951005008\n",
            "epochs 308/500\n",
            "train_loss 0.0007387882384454907\n",
            "epochs 309/500\n",
            "train_loss 0.0007217237757806965\n",
            "epochs 310/500\n",
            "train_loss 0.0007099259024092379\n",
            "epochs 311/500\n",
            "train_loss 0.000708802907655272\n",
            "epochs 312/500\n",
            "train_loss 0.0007028762152438679\n",
            "epochs 313/500\n",
            "train_loss 0.0007415086539203519\n",
            "epochs 314/500\n",
            "train_loss 0.0007513828597146017\n",
            "epochs 315/500\n",
            "train_loss 0.0007383930892028992\n",
            "epochs 316/500\n",
            "train_loss 0.0007257049242532351\n",
            "epochs 317/500\n",
            "train_loss 0.0007085296911121568\n",
            "epochs 318/500\n",
            "train_loss 0.000715644386076843\n",
            "epochs 319/500\n",
            "train_loss 0.0007319357133840753\n",
            "epochs 320/500\n",
            "train_loss 0.0007061227173829834\n",
            "epochs 321/500\n",
            "train_loss 0.0007188566565852864\n",
            "epochs 322/500\n",
            "train_loss 0.000741833523711013\n",
            "epochs 323/500\n",
            "train_loss 0.0007188135516147672\n",
            "epochs 324/500\n",
            "train_loss 0.0007086839953913535\n",
            "epochs 325/500\n",
            "train_loss 0.00070179725496469\n",
            "epochs 326/500\n",
            "train_loss 0.0007060085264436301\n",
            "epochs 327/500\n",
            "train_loss 0.00072254162703695\n",
            "epochs 328/500\n",
            "train_loss 0.0007092178813154777\n",
            "epochs 329/500\n",
            "train_loss 0.0007282736781995246\n",
            "epochs 330/500\n",
            "train_loss 0.0007000352987597056\n",
            "epochs 331/500\n",
            "train_loss 0.0007200154012341147\n",
            "epochs 332/500\n",
            "train_loss 0.0007445850158873492\n",
            "epochs 333/500\n",
            "train_loss 0.0007246639103469952\n",
            "epochs 334/500\n",
            "train_loss 0.0007026859975141502\n",
            "epochs 335/500\n",
            "train_loss 0.0007123738825206991\n",
            "epochs 336/500\n",
            "train_loss 0.0007328905936974932\n",
            "epochs 337/500\n",
            "train_loss 0.0007033752692632425\n",
            "epochs 338/500\n",
            "train_loss 0.0007128958013852443\n",
            "epochs 339/500\n",
            "train_loss 0.0007317211993781959\n",
            "epochs 340/500\n",
            "train_loss 0.0007055578600122045\n",
            "epochs 341/500\n",
            "train_loss 0.0007082726623315042\n",
            "epochs 342/500\n",
            "train_loss 0.0007237493033598344\n",
            "epochs 343/500\n",
            "train_loss 0.0006987697209209652\n",
            "epochs 344/500\n",
            "train_loss 0.0007090606306540898\n",
            "epochs 345/500\n",
            "train_loss 0.0007350352703920502\n",
            "epochs 346/500\n",
            "train_loss 0.0007031557589469193\n",
            "epochs 347/500\n",
            "train_loss 0.0007078354353981088\n",
            "epochs 348/500\n",
            "train_loss 0.0007262900892318395\n",
            "epochs 349/500\n",
            "train_loss 0.0006973321077206704\n",
            "epochs 350/500\n",
            "train_loss 0.0007097316657041141\n",
            "epochs 351/500\n",
            "train_loss 0.0007327855701732864\n",
            "epochs 352/500\n",
            "train_loss 0.0007031290144857772\n",
            "epochs 353/500\n",
            "train_loss 0.0007049753799774233\n",
            "epochs 354/500\n",
            "train_loss 0.0007154262288173255\n",
            "epochs 355/500\n",
            "train_loss 0.0006972203752844021\n",
            "epochs 356/500\n",
            "train_loss 0.0006907361034923908\n",
            "epochs 357/500\n",
            "train_loss 0.0007177100895560763\n",
            "epochs 358/500\n",
            "train_loss 0.0007456132292557454\n",
            "epochs 359/500\n",
            "train_loss 0.0007233245543290497\n",
            "epochs 360/500\n",
            "train_loss 0.0007013147688505133\n",
            "epochs 361/500\n",
            "train_loss 0.0006978114293198939\n",
            "epochs 362/500\n",
            "train_loss 0.0007000420002893624\n",
            "epochs 363/500\n",
            "train_loss 0.0007123357749270345\n",
            "epochs 364/500\n",
            "train_loss 0.0007292359662173215\n",
            "epochs 365/500\n",
            "train_loss 0.0007060883831854372\n",
            "epochs 366/500\n",
            "train_loss 0.0006934498137677765\n",
            "epochs 367/500\n",
            "train_loss 0.0006913798671107483\n",
            "epochs 368/500\n",
            "train_loss 0.0006883107671933826\n",
            "epochs 369/500\n",
            "train_loss 0.0007014754683598956\n",
            "epochs 370/500\n",
            "train_loss 0.0007549536197109032\n",
            "epochs 371/500\n",
            "train_loss 0.0007194601711666011\n",
            "epochs 372/500\n",
            "train_loss 0.0006904499636594954\n",
            "epochs 373/500\n",
            "train_loss 0.0007021938494956188\n",
            "epochs 374/500\n",
            "train_loss 0.0007290411275147715\n",
            "epochs 375/500\n",
            "train_loss 0.000699431627385355\n",
            "epochs 376/500\n",
            "train_loss 0.0006952409733031386\n",
            "epochs 377/500\n",
            "train_loss 0.00070625473760873\n",
            "epochs 378/500\n",
            "train_loss 0.0006967165897539011\n",
            "epochs 379/500\n",
            "train_loss 0.0007077298092166175\n",
            "epochs 380/500\n",
            "train_loss 0.0006906369329838017\n",
            "epochs 381/500\n",
            "train_loss 0.0006889537375911474\n",
            "epochs 382/500\n",
            "train_loss 0.0007197595737178004\n",
            "epochs 383/500\n",
            "train_loss 0.000727181344839471\n",
            "epochs 384/500\n",
            "train_loss 0.0007085810939481457\n",
            "epochs 385/500\n",
            "train_loss 0.0006888496057735055\n",
            "epochs 386/500\n",
            "train_loss 0.0006952419584264433\n",
            "epochs 387/500\n",
            "train_loss 0.0007128629953416932\n",
            "epochs 388/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4fb9547ebb3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs {}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-6b09478ff116>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-67575f8f3715>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    577\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phj4k1M3ZpUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = []\n",
        "batch_size = 1\n",
        "\n",
        "def Predictions():\n",
        "    model.eval()\n",
        "    for i in range(len(X)):\n",
        "        pred = model(torch.from_numpy(X[i]).unsqueeze(0).to(device).float())\n",
        "        preds.append(pred.detach().cpu().numpy().item())"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2Tw11ZqaRsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Predictions()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgXH9DAoaXSH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "5f6f9938-f568-4db8-b8ad-32b00daf7b6e"
      },
      "source": [
        "predictions = pd.DataFrame(preds, columns=['Energy'])\n",
        "predictions"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Energy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.233089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.233089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.233089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.233089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.233089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1816</th>\n",
              "      <td>0.291471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1817</th>\n",
              "      <td>0.290265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1818</th>\n",
              "      <td>0.288146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1819</th>\n",
              "      <td>0.292055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1820</th>\n",
              "      <td>0.290348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1821 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Energy\n",
              "0     0.233089\n",
              "1     0.233089\n",
              "2     0.233089\n",
              "3     0.233089\n",
              "4     0.233089\n",
              "...        ...\n",
              "1816  0.291471\n",
              "1817  0.290265\n",
              "1818  0.288146\n",
              "1819  0.292055\n",
              "1820  0.290348\n",
              "\n",
              "[1821 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah-Tbpgeas3D",
        "colab_type": "text"
      },
      "source": [
        "##Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G54-xFzaoCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.metrics import mean_absolute_error as MAE\n",
        "from sklearn.metrics import r2_score\n",
        "def MAPE(y_true, y_pred): \n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T90ApVZLaqWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MSE_CNN_LSTM = MSE(Y, predictions)\n",
        "RMSE_CNN_LSTM = MSE(Y, predictions)**(0.5)\n",
        "MAE_CNN_LSTM = MAE(Y, predictions)\n",
        "MAPE_CNN_LSTM = MAPE(Y, predictions)\n",
        "R2_CNN_LSTM = r2_score(Y, predictions)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5inetZjbOtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Metric_CNN = {\n",
        "    'MSE_CNN-LSTM':MSE_CNN_LSTM,\n",
        "    'RMSE_CNN-LSTM':RMSE_CNN_LSTM,\n",
        "    'MAE_CNN-LSTM':MAE_CNN_LSTM,\n",
        "    'MAPE_CNN-LSTM':MAPE_CNN_LSTM,\n",
        "    'R_Squared_CNN-LSTM':R2_CNN_LSTM,\n",
        "}"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T71jS4CsbboK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f8803cc4-364b-40de-b2df-1fcb37cccc0e"
      },
      "source": [
        "Metric_CNN"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MAE_CNN-LSTM': 0.013042370644863108,\n",
              " 'MAPE_CNN-LSTM': 5.974808045040905,\n",
              " 'MSE_CNN-LSTM': 0.0006731616646988277,\n",
              " 'RMSE_CNN-LSTM': 0.02594535921313921,\n",
              " 'R_Squared_CNN-LSTM': 0.9559110699946395}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}